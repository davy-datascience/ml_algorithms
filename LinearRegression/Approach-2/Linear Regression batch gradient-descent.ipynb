{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Linear Regression batch gradient-descent.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyNEfKsG0tNNjf06qajd7OIX"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"F0QzuT7LPKps","colab_type":"text"},"source":["# Linear Regression - multi variable (using batch gradient descent)"]},{"cell_type":"markdown","metadata":{"id":"Y02rlBSiKDfK","colab_type":"text"},"source":["## Intro\n","\n","Here batch gradient descent is implemented for linear regression. The theory part was brought to me by Andrew Ng in the [Machine Learning](https://www.coursera.org/learn/machine-learning/) course which I strongly recommend. \n","\n","I implemented the algorithm in python.\n","\n","We are seeking to build a regression function with the hypothesis below: ![hypothesis equation](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/hypothesis.png)\n","\n","n = number of features\n","\n","![feature values](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/x1x2x3.png) = the values of each feature\n","\n","We can represent the feature values as the following column vector:\n","\n","![x vector](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/x.png)\n","\n","![tetha parameters](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/tetha_list.png) = parameters (coefficients) of the hypothesis equation\n","\n","We can represent those parameters as the following column vector:\n","\n","\n","![theta vector](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/theta.png) \n","\n","The hypothesis can be represented as the multiplication of those matrix:\n","\n","![hypothesis equation](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/hypothesis_matrix.png)\n"]},{"cell_type":"markdown","metadata":{"id":"wZE0D9wXAonJ","colab_type":"text"},"source":["## Cost function & Gradient descent\n","\n","The cost function represents the error: how far are the predicted values from the actual values?\n","\n","The cost function choosen here is the mean squared error:\n","\n","![mean squared error](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/mse.png)\n","\n","Our goal is to minimize the cost function. To do so, we update simultaneously each parameter tetha by substrating a small portion of the partial dervative of the cost function:\n","\n","![gradient descent](https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/img/gradient_descent.png)\n","*italicized text*\n"]},{"cell_type":"markdown","metadata":{"id":"nysrUb7R7y7B","colab_type":"text"},"source":["## Implementation\n","\n","Run the following cell to import all needed modules, you must have opened this document on Google Colab before doing so: <a href=\"\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"code","metadata":{"id":"wYli7HrM7v68","colab_type":"code","colab":{}},"source":["import pandas as pd\n","from pandas.core.frame import DataFrame\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_absolute_error\n","from my_functions import normalize, feature_engineering, predict\n","import progressbar"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TPgBme3Y8rZq","colab_type":"text"},"source":["Run the following cell. It contains the functions that will be used in the program:"]},{"cell_type":"code","metadata":{"id":"U-ZA6F3e8sX-","colab_type":"code","colab":{}},"source":["def normalize(X):\n","    ''' Apply mean normalization, so each feature is at the same scale '''\n","    X_normalized = pd.DataFrame()\n","    avg_X = X.mean()\n","    range_X = X.max() - X.min()\n","    \n","    for i in range(len(X.columns)):\n","        X_normalized[i] = (X.iloc[:, i] - avg_X[i]) / (range_X[i] if range_X[i] != 0 else 1)\n","    X_normalized.columns = X.columns\n","    return X_normalized\n","\n","def feature_engineering(X):\n","    ''' Select interesting features and create new calculated features '''\n","    features = ['LotArea', '1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'KitchenAbvGr', 'FullBath', 'HalfBath', 'BsmtFullBath', 'BsmtHalfBath', 'PoolArea']\n","    \n","    X = X[features].copy()\n","    X[\"FlrSF\"] = X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]\n","    X[\"Bath\"] = X[\"FullBath\"] +  X[\"HalfBath\"] + X[\"BsmtFullBath\"] + X[\"BsmtHalfBath\"]\n","    \n","    X = X.drop(columns=['1stFlrSF', '2ndFlrSF', \"FullBath\", \"HalfBath\", \"BsmtFullBath\", \"BsmtHalfBath\"])\n","    \n","    return X\n","\n","def predict(X, tetas):\n","    ''' Predict with the model (tetas)\n","        X input can be either a Series for a single example to predict or a DataFrame\n","    '''\n","    X = X.copy() # So we do not alter the input dataframe X\n","    \n","    # Check if the input is either DataFrame or a Series\n","    if isinstance(X, DataFrame):\n","        X.insert(0, \"X0\", 1)\n","        y_pred = pd.Series()\n","        \n","        for i in range(len(X.index)):\n","            y_pred = y_pred.append(pd.Series(tetas.dot(X.iloc[i])))\n","        \n","        return y_pred\n","    \n","    else:\n","        X = pd.Series([1]).append(X)\n","        return tetas.dot(X)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m6T8uiBx82Tw","colab_type":"text"},"source":["Run the following cell to launch the linear regression program:"]},{"cell_type":"code","metadata":{"id":"LEyPEuDy9FHS","colab_type":"code","colab":{}},"source":["# Set the learning rate and the number of iterations\n","learning_rate = 0.1\n","epochs = 50\n","\n","# Read the data\n","dataset = pd.read_csv(\"https://raw.githubusercontent.com/davy-datascience/portfolio/master/LinearRegression/Approach-2/dataset/house_pricing.csv\")\n","\n","# Separate the dataset into a training set and a test set\n","train, test = train_test_split(dataset, test_size = 0.2)\n","\n","# Separation independent variable X - dependent variable y for the train set & the test set\n","# Do specific feature engineering & normalize X\n","X_train = feature_engineering(train)\n","X_train = normalize(X_train)\n","y_train = train[\"SalePrice\"]\n","\n","X_test = feature_engineering(test)\n","X_test = normalize(X_test)\n","y_test = test[\"SalePrice\"]\n","\n","# Arbitrary begin with all tetas = 1\n","tetas = np.ones(X_train.columns.size + 1)\n","\n","for i in progressbar.progressbar(range(epochs)):\n","    # Create a variable new_tetas so we can update simulatneously variable tetas\n","    new_tetas = []\n","    \n","    # Iterate over all tetas to calcuate their new values that will be updated simulteously\n","    for j in range(len(tetas)):\n","        sum_cost = []\n","        # Iterate over all training data\n","        for i in range(len(X_train.index)):\n","            # Calculate the predicted value of the data row with our regressionFunction\n","            h = predict(X_train.iloc[i], tetas)\n","            y = y_train.iloc[i]\n","            x = 1\n","            if j != 0:\n","                x = X_train.iloc[i, j - 1]\n","            sum_cost.append((h - y) * x)\n","        new_tetas.append(tetas[j] - learning_rate * (1/len(X_train.index)) * sum(sum_cost))\n","\n","    # Simultaneous update of thetas\n","    tetas = np.array(new_tetas)\n","\n","# Predict the test set with my model and see\n","y_pred = predict(X_test, tetas)\n","print(\"MAE for my model: {}\".format(mean_absolute_error(y_pred, y_test)))\n","\n","# Predict the test set with the sklearn algorithm\n","from sklearn.linear_model import LinearRegression\n","regressor = LinearRegression()\n","regressor.fit(X_train, y_train)\n","y_pred2 = regressor.predict(X_test)\n","print(\"MAE for the algorithm of the sklearn module: {}\".format(mean_absolute_error(y_pred2, y_test)))"],"execution_count":0,"outputs":[]}]}